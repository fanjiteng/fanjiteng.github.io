<h1 style="text-align:center;">从关系到多维地图：一种SQL到HBase的转换方法论</h1>

### 摘要

本文描述了一种将依赖于关系型数据库的应用程序迁移到HBase后端的方法论。我们的方法包括（a）将SQL数据模式迁移到HBase的数据模式迁移步骤，以及（b）将应用程序SQL查询转换为等效的HBase API调用序列的转换。我们的数据模式迁移方法依赖于一组HBase组织指南，以驱动四步数据模式转换过程。其中一些指南是查询无关的：我们根据相关文献中有关HBase组织期望属性的定义来制定它们。其他指南是查询感知的：我们设计它们以在转换过程中纳入从查询日志中提取的数据访问路径，以提高转换的质量和最终HBase存储库的访问效率。 我们的转换方法维护了源模式和目标模式之间的映射，该映射用于创建等效于关系数据库中SQL查询的HBase API调用序列。我们通过案例研究和全面的性能评估来说明和验证我们的方法。

### CCS 概念

**• 信息系统 → 并行和分布式数据库管理系统；**
非关系型引擎的查询语言；

### 关键词

分布式数据库，数据转换，程序转换，数据库设计和建模，HBase，NoSQL。

### 1.引言

​		自上世纪70年代初技术诞生以来，关系型数据库已被几乎所有软件应用所采用。然而，如今，在现代应用程序中收集的大量数据正变得越来越具有挑战性，对关系数据库系统提出了越来越高的要求。云计算的出现大大降低了部署和存储成本，但只有系统能够轻松地在多个服务器上进行水平扩展，才能充分利用云技术。不幸的是，关系数据库系统传统上驻留在一台服务器上，只能进行垂直扩展，需要增加该服务器的计算和存储能力。此外，关系模型表现出缺乏灵活性的特点，这与流行的敏捷软件开发实践相冲突，这些实践暗示了对模式演进的需求；因此，当软件系统的信息模式需要演进时，严格的关系模式可能会成为负担。
尽管商业关系数据库管理系统（如Teradata、Greenplum和Netezza）据报道能够成功管理大型数据集，但总体上，关系数据库存在三个限制 [20]。

​		首先，它们的容错机制无法处理复杂环境中的频繁故障。其次，它们的弹性降低了动态重新配置的能力，以响应不断变化的负载要求。第三，关系数据模型对于存储非结构化（或半结构化）数据的刚性，而这通常是这些系统收集的数据类型。此外，商业关系数据库管理系统可能价格昂贵，而开源系统（如MySQL）在可扩展性和性能方面远远落后 [14]。

​		因此，数据量的增加和关系数据库的限制迫使像Facebook、亚马逊和谷歌这样的大公司开发自己的存储系统，从而产生了一种新型数据库，称为NoSQL数据库。这些数据库通常简化了关系数据库的要求，以换取良好的可用性、弹性和可扩展性，通过在云计算平台上轻松部署。根据其数据模型，NoSQL数据库通常分为四种主要类型：图数据库、键值存储、列族存储和文档存储 [12]。HBase是一种列族存储，是Google Big Table 的开源对应物 [5]。虽然HBase使用了对于关系数据库开发人员来说熟悉的术语来描述其数据模型（如表、行和列），但其底层模型是一个排序的多维映射，带有单元格。这些单元格的每一个都通过其行键、其列族名称、其列名称和其版本来指定 [1]。HBase中的记录存储为按其行键排序的键值对列表，与关系数据库不同，HBase仅对行键进行索引。因此，行键决定了数据访问的性能。HBase已被许多大公司成功使用，例如，Facebook使用HBase管理来自数百个应用程序的海量数据 [4]。

​		受到将传统的基于关系型数据库的应用程序迁移到可扩展的云计算平台的挑战的启发，我们提出了以下方法：(a) 从关系结构到HBase的数据模式转换过程，以及(b) 从SQL到一系列HBase API调用的查询转换算法。数据模式转换是一种启发式方法，受到Gupta等人关于将关系数据转换为Hive的工作的启发。我们的方法在[23]中进行了详细介绍，包括一个包括系统化去规范化、索引转换和编码以及创建物化视图的四步数据模式转换过程。该方法通过从查询日志中推断数据访问路径，使用索引使用信息和联接表的信息，最小化了冗余。在[23]中，我们通过案例研究验证了模式和数据映射，而在本文中，我们通过扩展仪器使实验结果更加准确，并阐明了性能变化的原因。查询转换算法是本文的主要贡献，它使用模式转换中创建的映射，以定义一个将连接SQL查询转换为一系列HBase调用的过程。SQL查询的转换包括四个步骤。首先，算法根据行键和SQL查询中可用的谓词选择适当的表。第二步是根据模式转换过程中的指南对行键进行编码。然后，根据行键的完整性，算法创建对正确的读取方法的调用，并根据投影列添加相应的列族。最后，算法迭代结果并从单元格中提取值。

​		本文的剩余部分组织如下。第2节回顾了此工作的背景，包括模式转换、索引结构以及面向列族存储的查询转换。第3节解释了从关系数据库管理系统(RBDMSs)到HBase的数据模式转换的启发式方法。在相同的部分中，我们展示了如何通过挖掘查询日志中的信息来增强转换。第4节介绍了我们的SQL到HBase翻译算法。第5节报告了我们的评估研究。最后，在第6节中，我们总结了我们的贡献并讨论了未来工作的计划。

### 2.背景

​		最近，一些关于将关系数据迁移到NoSQL数据库的研究已经出现。在[16]、[22]和[9]中，作者提出了从RDBMS迁移到NoSQL存储的转换，这些转换基于对原始关系进行去规范化，然后进行一系列特殊用途的模式修改，大多数情况下，这些修改是不适用于其他ER模型的。
这些先前的研究已经确立了模式去规范化的重要性，这也是我们数据迁移过程中的关键步骤之一。然而，我们工作的一个独特特点和贡献是，除了源数据的ER模式之外，它还考虑了应用程序数据访问模式（通过应用程序日志获取）来优化HBase模式，并减少由于去规范化而产生的存储要求和数据冗余。

​		与迁移方法学并行的是，一些关于如何设计HBase数据编码的研究也在进行中。
最值得注意的编码策略之一是将时间序列数据组织成与周期对应的“桶”，例如在OpenTSDB [24]中提出的。OpenTSDB是一个分布式可扩展的时间序列数据库，构建在HBase之上。OpenTSDB提供了一个设计用于支持数据局部性的数据模型，因此可以获得良好的查询执行性能。在我们先前的工作中 [10]，我们提出了一个数据模型，其中数据点可以直观地表示为行、列和版本，版本维度代表序列ID，可以是单调递增的时间戳或用于临时序列数据的快照标识符。

​		地理空间数据集也需要特殊的组织方式。地理空间数据集中的数据点通常是多维的，包括它们的坐标和数据点上的域对象的描述。空间驱动的组织的一个示例是使用等大小的矩形单元格对矩形域进行分区的网格 [8]。一些作者，例如[18]和[11]，已经提出了大规模地理空间数据集的数据模型，但据我们所知，仍然没有一种系统化的方式可以让应用程序开发人员在数据模式迁移过程中遵循，这是我们在这项工作中要解决的主要问题之一。

​		最后，学术界和工业界已经做出了许多努力，以便更容易地访问NoSQL数据存储中的数据。例如，Unity [15]架构允许SQL查询自动转换并使用底层NoSQL源的本机API执行。类似地，Zhang等人 [25]定义了一个字典结构来描述数据库映射，并提供了一个SQL引擎，将标准SQL语句转换为对NoSQL数据库的各种类型的访问。在[19]中，作者提出了SQL ++，它由基于JSON的映射模型和一个与SQL完全向后兼容的查询语言组成。在[7]中，Cur´e等人提出了一个具有基于本体的数据访问机制（基于描述逻辑的OBDA）的映射模型，利用语义知识，并建议使用SPARQL作为NoSQL数据存储的查询语言。然而，上述项目中没有一个描述了将现有查询实现转换的方法论，这是任何SQL到NoSQL迁移项目的关键要求。最接近满足这一要求的工作是JackHare [6]，在这项工作中，作者提出了一种将SQL查询转换为处理数据的MapReduce作业的系统方法，使用了HBase。然而，用于将转换后的HBase模型映射到原始关系模型的数据模型不适用于更复杂的关系模式。

​		与此同时，行业也提出了其他方法，例如Apache Phoenix [2]和Cloudera Impala [3]等项目，它们在客户端和HBase之间提供了一个关系层。Phoenix是Salesforce.com最初开发的技术，用于提供API以使用嵌入式JDBC驱动程序运行SQL，允许客户端连接到HBase。而Impala是一个SQL引擎，可以运行在HDFS或HBase上。Impala是作为Hadoop生态系统的一部分从头开始集成的，并利用了相同的灵活文件和数据格式、元数据、安全性和资源管理框架。这些方法的一个关键劣势是它们在源和目标模式之间的一般映射阻碍了为提高性能而必要的特殊编码的创建。当前方法的另一个缺点是缺乏统一和一致的方式来访问重复的信息。通常，在HBase中，数据会被重复以允许不同的数据访问模式，但现有工具没有表达力强的模式映射，以帮助开发人员选择适合给定访问模式的正确表和编码行键。在我们的方法中，迁移过程在表和属性级别创建映射，这些映射用于匹配SQL查询中的谓词和HBase表中的行键的有效数据访问模式。

### 3.迁移方法

​		关系数据库和列族数据存储是根本不同的技术，具有不同的设计属性，必须在应用程序设计中加以考虑。数据模型的差异，加上缺乏通用的元模型模式定义和映射，使得管理者和开发人员更难移植应用程序或利用现有的知识。受到从关系数据库管理系统(RDBMSs)到HBase的自动迁移的挑战的启发，我们开发了一个四步启发式方法学，用于将关系模型映射到多维映射，详细报告在[23]中。在本节中，我们非常简要地总结了这个方法，为解释我们的算法奠定了基础，用于将SQL查询迁移到HBase API调用序列。

**第一步：关系去规范化** 规范化是关系模式设计的关键原则，旨在保留存储空间并简化数据更新。然而，HBase不支持原生的连接；相反，提供等效功能的常见技术是对数据进行去规范化，以便应用程序中感兴趣的所有数据都可以在同一张表中找到。因此，对于一对一关系，我们的转换方法将其中一张表合并到另一张表中。如果两张表之间存在多个关系，则在双方都进行合并。对于一对多关系，我们在两侧进行合并。当将多方合并到单方时，一种嵌套实体技术指导将一对多记录存储在单个行中。为了在单行中有多个记录，每条这样的记录都被分配一个列修饰符，其中存储了其识别属性，而非识别属性的其余部分被添加到记录值中。

**第二步：扩展表合并** 有时，应用程序需要通过一系列连续的连接来合并多个表中的数据，在HBase中这将导致大量性能下降。为了减轻这种影响，我们的方法递归地尝试合并“相邻”的表。我们将关系模式视为一个图，其中表是顶点，关系是边。然后，通过以每张表为根的广度优先搜索(BFS)，将所有可达的表合并到根表中。实际上，第二步是第一步的扩展，可以视为深度为一的BFS。合并过程继续，直到达到以下条件之一：(a)达到预定义的级别，或(b)关系是一对多关系，合并是朝着多的部分进行的，并且前面的步骤还涉及一对多关系，将多表合并到一表中。本质上，该过程受限制，使得不存在超过一个级别的嵌套实体。

**第三步：关键编码** 在HBase表中，行键设计对于数据访问效率至关重要。与支持多个索引的关系数据库不同，HBase表只提供一个基于行键的单一索引，实际上确定了读操作的性能。我们在我们的方法中使用了几种设计行键的技术，这些技术针对不同的访问模式进行了优化。对于分类数据，我们识别关系表的索引属性，并通过连接这些属性的值来创建HBase中的行键。对于时间序列数据，我们设计了一个行键以加速时间范围查询，方法是将属于给定周期内的时间戳的数据项组合在一起。在此编码中，将时间戳向下舍入到最近的日/小时/分钟（无论选择的时间单位是什么），作为行键前缀编码，与感兴趣的其他属性连接起来。时间戳的余下部分(舍入之前)成为列修饰符的一部分。对于空间数据，我们设计了一个行键，以提高最近邻查询的性能，方法是使用地理哈希。地理哈希是一种分层的空间数据结构，将空间分割成网格形状的桶，以确保在物理空间中彼此靠近的点也在HBase表中彼此靠近。

**第四步：基于索引的视图** HBase仅限于每个表一个行键。在必须支持多个访问路径的情况下，应该基于二级索引创建物化视图，正如在[21]中提出的那样。我们的模式迁移方法首先识别关系表的唯一和非唯一索引。对于唯一索引，索引属性成为视图行键，遵循上面讨论的指南。对于非唯一索引，为了避免冲突，可以将非唯一索引的属性和主键属性连接起来，形成视图的唯一行键。作为构建HBase表的全面副本的替代方法，可以创建包含二级索引作为行键的高表，以及主键值作为单个单元格值。

**通过查询感知的转换优化HBase模式**
		定义HBase表的设计的关键方面是应用程序的典型数据库访问模式。我们上面讨论的去规范化方法在第一步之后导致数据重复程度很高。然而，通过检查原始应用程序的查询日志，我们可以确定应用程序的典型数据访问模式，并可以优化HBase表组织，以有效支持这些模式，同时尽量减少数据重复。

​		在原始的第一步中，我们考虑所有的A-B表对，其中表A被表B引用。在了解了访问模式的情况下，应该只考虑在同一SQL查询中同时出现的表对。

​		此外，如果A-B表对仅在总是使用B的索引或属性作为过滤器的查询中出现，则不应将B合并到A中。如果A的属性或索引从未被使用，那么它可能会被合并到通过连接提取信息的另一张表中。在这种情况下，A是可能要删除的候选项，除非该表在与单个表相关联的查询中使用。

**Dell DVD商店示例**
		作为验证我们从基于SQL的软件系统到基于NoSQL的系统的迁移工作的试验场，我们选择了Dell的DVD商店测试应用程序[13]。这是一个用于评估戴尔服务器性能和可伸缩性的电子商务测试应用程序。该应用程序由数据库模式、加载脚本和一个Web应用程序组成，使用户能够登录商店、浏览和购买DVD。因此，它构成了一个完整的软件系统，包括所有相关资产，是一个理想的试验场。

​		关系模式如图1所示，其中包括客户、产品、订单、订单行、类别、库存和重新订购等表。在应用自动迁移后的结果HBase模式如图2所示。

![image-20240331143606819](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331143606819.png)

图1：DVD商店数据库的实体关系图

![image-20240331143856038](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331143856038.png)

图2：迁移后的HBase模式

### 4.从SQL查询到HBase API调用

​		在模式级别，我们已经定义了一个从关系到HBase的映射，该映射捕获了一组关系应该如何存储到HBase中的多维映射。在本文中，基于这个映射算法，我们定义了一个将SQL查询转换为HBase API调用的过程；更具体地说，我们的过程适用于联结的SQL查询，对应于where条件中仅使用原子条件的select-from-where查询。

​		将SQL表达式转换为HBase调用分为四个步骤。

- 第一步根据HBase表中的rowkey和SQL查询中的谓词选择适当的HBase表。
- 第二步从谓词的值和查询的类型构建rowkey。
- 然后，第三步创建相应的HBase调用（根据前一步的rowkey），并根据select子句中找到的列添加列族。
- 最后，第四步迭代结果并从单元格中提取值。

​		HBase是用Java实现的，并且具有一个Java本机API，提供对表中存储的数据的编程访问。使用Java API进行典型的读操作始于通过HTableInterface对象打开与表的连接（步骤1）。然后，考虑到在HBase中所有数据都以字节数组的形式存储为原始数据，将rowkey进行编码并转换为字节数组（步骤2）。Java API实现了一个实用类Bytes，提供了方便的方法来将多种Java数据类型转换为字节数组，反之亦然。随后，向数据库发出读取方法调用（步骤3）。在HBase中有两种读取数据的方式：Get和Scan。Get操作返回单行并需要唯一的rowkey作为输入。另一方面，Scan返回一系列连续的行，并且需要两个rowkey作为输入，一个是startkey，一个是endkey，指示要返回的表的部分的边界。此外，在Scan操作中，可以指定一个Filter，在数据库节点中执行谓词，以确定是否应返回记录。最后，对读取方法的调用返回Result或ResultScanner实例，分别用于Get和Scan方法。这些实例包含所指定行的所有列，可以从字节数组提取并转换为相应的Java类型（步骤4）。

#### 4.1 表选择

​		将SQL查询迁移到等效的一组HBase API调用的过程中的第一个任务是识别可能为SQL查询所代表的数据访问模式提供最佳性能的HBase表。

​		给定一组关系表R，每个表ri与HBase模式H中的零个或多个表相关联。对于H中的每个表 h<sub>j</sub> ，定义行键K为其源关系表中多个列的串联{k1，..，kn}，如第3节的步骤3中所定义的那样。

​		在这一步中，算法1遍历SQL查询中from子句中提到的表，并收集从这些关系表生成的H中的表的集合（行4-7）。

​		这是感兴趣的表的超集，因为只有其中一些支持查询的数据访问模式。从返回的表中，算法从左到右检查rowkey K中的每个列ki是否通过SQL查询中对应的谓词引用（行8-14）。算法仅保留从开始位置连续的关键列匹配的计数，因为通过后缀过滤行键将需要进行完整的表扫描。然后，选择具有最大连续匹配计数的表 h<sub>j</sub> 作为接下来的翻译中要使用的表（行16-19）。

​		如果返回的表集为空，或所有表的计数器为零，则没有有效的方法来转换查询，过程将停止，暗示查询将需要进行完整的表扫描。例如，当SQL查询中的谓词不使用索引列时，可能会发生这种情况。

![image-20240331144111658](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331144111658.png)

![image-20240331144132892](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331144132892.png)

​		考虑以下查询的示例。

![image-20240331144245118](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331144245118.png)

​		算法首先识别与关系表"orders"和"orderlines"映射的表，这些表是HBase表"orders"、"orders ix cust"和"orders ix date"。在这些HBase表中，只有"orders"表的rowkey与SQL查询的谓词匹配。因此，选择表"orders"，并生成以下HBase调用。

![image-20240331144338840](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331144338840.png)

![image-20240331144506162](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331144506162.png)

![image-20240331144518566](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331144518566.png)

​		通过选择的HBase表的rowkey，算法可以跟踪行键的后缀，该后缀未被查询谓词正确匹配的情况，当行键和SQL谓词之间存在部分匹配时。后缀将有助于通过填充剩余的rowkey字节创建适当的起始键和结束键，从而执行高效的扫描。

#### 4.2 键的创建

​		一旦目标表 h<sub>j</sub> 被确定，任务就是对在查询的谓词中提到的键或键的部分进行编码。

​		算法2遍历了行键K中的键元素{k<sub>1</sub>, .., k<sub>n</sub>}的列表，并将它们的值连接在谓词中。
如果谓词是一个范围谓词的一部分，例如，当在谓词中使用BETWEEN运算符时，那么它记录了行键的起始值和结束值（行6-7）。如果没有范围谓词，则行键的起始值和结束值将相同（行9-10）。

​		接下来，如果查询在空间列上有范围谓词，算法会生成代码来分割执行并创建九个相邻的查询（行17）。最后，它生成了行键的变量，其中起始值和结束值与一个填充了大小等于前一步计算得到的后缀的零字节数组连接在一起（行19-20）。

​		例如，在上述查询中，行键将以谓词中提到的值1创建，然后将生成以下代码。

![image-20240331144628734](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331144628734.png)

​		空间查询，例如下面所示的查询，更加复杂。翻译器必须从位置范围的中点生成九个查询。地理哈希查询的精度是根据范围（由纬度和经度两个范围定义）推断的，在本例中是4字符精度。然后，算法计算键的起始和结束，并附加后缀（对于谓词中不包含的整数，这里是4字节）。生成的用于HBase Java API的代码如下所示：

![image-20240331144744070](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331144744070.png)

#### 4.3 HBase 调用执行

​		在这一步中，算法3使用从前面步骤提取的信息生成与SQL查询匹配的适当的HBase调用。更具体地说，如果查询中的谓词覆盖了行键的所有列，并且使用等号运算符，那么调用是一个Get操作（行3）。但是，如果谓词没有完全覆盖行键，或者至少有一个谓词使用了不等号运算符，例如LIKE或大于，则HBase调用是一个Scan操作（行5）。
​		此外，算法遍历查询的选择子句中找到的列，提取相关的列族，并生成必要的代码以将列族包含在HBase查询中（行7-9）。
​		在第一步中提供的示例中，查询的谓词覆盖了行键的所有列。因此，生成的代码包含一个Get调用，如下所示：

![image-20240331144840421](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331144840421.png)

#### 4.4 内容提取

​		最后，算法4通过识别列族和列限定符的特定编码来提取在SQL查询中投影的信息。
有两种特殊情况：嵌套实体和时间序列行键。当列族属于嵌套实体时，列限定符的一部分将作为前缀进行标识。
​		因此，为了获取单元格的值，算法需要从列限定符中提取前缀，并评估列限定符的其余部分是否与SQL查询中投影的列相同（行3-6）。如果表 h<sub>j</sub> 的行键中包含一个时间序列列，那么为了提取单元格的内容，首先需要提取列限定符中由我们的时间序列编码生成的时间的剩余部分。
然后，算法评估列限定符的其余部分是否与SQL查询中投影的列相同，就像我们处理嵌套实体一样（行7-13）。
​		例如，在第二步中提供的第二个示例中，我们只需要获取客户的名字，由于列族customer不是嵌套的，或者行键不是基于时间序列编码的，因此生成如下所示的代码。

![image-20240331145024107](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331145024107.png)

第一个例子更为复杂，因为其中一个列族（orderlines）来自于一个嵌套实体。在这种情况下，算法必须提取列限定符的部分，并检查名称是否与查询中投影的列相同。在下面的代码中，我们可以看到列名称丢弃了列限定符的前4个字节。

![image-20240331145048758](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331145048758.png)

### 5.评估

​		文献中使用了三种不同类型的度量标准来评估数据库迁移过程：（a）模式信息保留，（b）数据等效性，以及（c）系统效率[17]。在[23]中，我们通过将自动迁移过程的结果与专家和类似系统获得的结果进行验证，来评估模式信息的保留。同样，我们通过观察源模式和其相应的目标模式之间的差异，根据加载到两个数据库中的数据样本的查询结果，来评估数据等效性。

​		本文的贡献是我们的算法，用于将SQL查询转换为相应的一系列HBase API调用，根据我们的从关系到HBase模式转换算法。为了验证这一贡献，我们进行了两项实验。第一项实验评估了戴尔应用程序的原始SQL查询与相应的HBase API调用序列之间的相对性能（吞吐量）。第二项实验通过比较我们的翻译过程自动生成的代码与由HBase专家开发的代码，来评估翻译过程的“自然性”。

​		以下是原始戴尔应用程序中找到的SQL查询的一个超集。

查询1（单一-精确）：查找具有变量ID的客户的信息。
查询2（多个-精确）：查找具有变量ID的产品和库存信息。
查询3（前缀扫描）：查找姓氏以特定前缀开头的所有客户，例如“STRO”。
查询4（单个-连接）：检索具有变量ID的订单以及其中包含的行项目的信息。
查询5（多个-连接）：检索具有变量ID的订单的完整信息，包括行项目和产品信息。
查询6（单个-插入）：插入一个新的客户。
查询7（多个-插入）：插入具有五个项目的新订单。
查询8（时间扫描）：检索在随机时间窗口（1小时）内下的订单。
查询9（地理扫描）：检索附近具有地址的任何客户的信息，附近指的是在随机位置点周围（大约5公里）。

​		大数据应用程序面临的主要挑战在于将系统扩展到支持高并发读写操作的高吞吐量。为了研究我们的迁移过程在这种情况下的性能，本文通过使用一个扩展的工具基础设施来增强[23]的实验，测试了原始SQL数据库和由我们迁移过程产生的HBase实现在高读写负载下的相对性能，并分析了性能变化的潜在原因。此外，我们将通过我们上述自动查询转换过程产生的HBase代码的性能与由HBase专家开发的代码进行比较。

#### 5.1 实验设置

​		实验涉及一组应用于15GB Dell DVD Store [13]关系数据库和我们原型生成的目标数据库的查询。实验中使用的查询集是测试应用程序中查询的一个超集，仅增加了对特殊编码的查询。我们的原型是一个Java程序，实现了第3节中描述的方法，并自动将数据加载到目标数据库中。

​		实验在一个由七个节点组成的集群上进行，在Cybera的Rapid Access Cloud中运行了七个虚拟机。虚拟机运行64位Ubuntu 12.04，具有4个核心，8GB RAM和40GB磁盘。我们使用了Cloudera的CDH 5.3.1中打包的HBase版本0.98.6，以及MySQL Cluster 7.7.3。我们选择了MySQL Cluster，因为它在低成本、商品硬件上提供了水平可扩展性，就像HBase一样，但是使用的是关系模型。我们的配置中有一个节点运行主进程，六个节点作为数据节点。每个运行节点的HBase和MySQL都分配了3GB的堆大小，没有副本。在HBase中，使用GZIP对数据进行了压缩。原型被配置为在步骤4上生成（完整复制的）材料化视图，具有1小时精度的时间序列数据集，列限定符不进行字符串压缩。

​		数据库包含2000万个客户、1200万个订单（平均每个订单5行）和10万个产品。在HBase中，每个客户及其重复的表的大小在3.5到5GB之间，每个订单及其重复的表在4到5GB之间，产品为8MB，总共30GB。因此，每个HBase数据节点平均有5GB的数据，超过了每个节点分配的内存。

#### 5.2 SQL-HBase 吞吐量比较

​		在这些实验中，我们将数据库系统置于高负载下，并通过Ceilometer收集CPU和RAM使用情况、网络流量以及文件读写等信息。Ceilometer是一个旨在提供OpenStack核心组件跨资源测量的唯一接触点的项目。对于当前版本Ceilometer中未包含的度量标准，我们通过SAVI战略网络的贡献者扩展了其功能。

表1：吞吐量实验的经过时间（以秒为单位）和平均每个操作的时间（以毫秒为单位）。

![image-20240331145346486](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331145346486.png)

​		我们将测试用例分为六个工作负载，由第5.1节中提到的查询子集表示。以下是每个工作负载的特征描述，并讨论了MySQL Cluster和HBase的结果。表1显示了MySQL Cluster（在磁盘上）和HBase版本的工作负载的记录时间（以秒为单位）和平均每个操作的时间（以毫秒为单位）。

*读取工作负载。*
		前两个工作负载是读取密集型的，它们基于第一个和第二个查询：SINGLE-EXACT和SINGLE-MULTIPLE。在第一个查询中，我们使用表customers获取具有随机标识符的客户的所有信息。记录在整个数据集中均匀概率地选择。在MySQL中，此工作负载一次从表customers获取一百万条记录，逐行获取。类似地，在HBase中，工作负载提取表customers中客户列族中包含的所有列的内容。同样，第二个查询将表products和inventory连接起来，以获取一百万个随机选择的产品的库存信息。在MySQL中执行此查询时，两个表根据产品ID定义的索引进行连接；在HBase中，对于给定的行键，查询获取同一张表中两个列族的内容。
		在执行读取工作负载时，我们发现了可以通过它们的分区策略来解释的差异。
MySQL Cluster默认实现了水平数据分区（自动分片），其基于表上的主键的哈希算法。相反，HBase表被分割成连续的区域，并且区域被分配给数据节点。如图3所示，在主节点中消耗的资源明显多于数据节点。在数据节点中，我们发现在MySQL Cluster中，负载分布更均匀，而在HBase中，只有一个节点包含查询表的区域。

![image-20240331145428215](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331145428215.png)

图 3: 查询 MULTIPLE-EXACT 的读取工作负载的 CPU 利用率。左侧表示 MySQL Cluster 的 CPU 利用率，右侧表示 HBase 的 CPU 利用率。

​		在第一个工作负载中，HBase 的吞吐量最高，超过了 655 ops/秒，而 MySQL Cluster 仅约为 100 ops/秒。对于第二个工作负载，结果类似。HBase 的吞吐量最高，超过了 1.1K ops/秒，而 MySQL Cluster 仅为 710 ops/秒。
尽管 MySQL Cluster 保持了负载的均匀分布，将一起访问的数据保持在一起，像 HBase 一样，会产生更有效的策略，因为在稍后对同一表的一些后续查询中，所需的块已经加载到缓冲高速缓存中。

*前缀工作负载。*

​		在这个工作负载中，我们使用第三个查询 PREFIX-SCAN，它检索从表 customers 中选择的以三个字母前缀开头的记录。前缀是随机生成的，具有均匀分布。在 MySQL 中，我们使用带有通配符字符的 LIKE 关键字提交了 50,000 条查询。对于 HBase，我们必须注意，姓氏没有固定的长度，这使得查找候选行变得复杂。在这种情况下，我们使用 PrefixFilter，这是一种特殊类型的 RowFilter，有助于计算停止行。PrefixFilter 足够聪明，可以找到扫描的结尾，但不足以提前跳到第一个匹配的起始行，这必须提供。

​		工作负载的执行显示了与读取工作负载类似的行为，其中在 MySQL 中，负载均匀分布到数据节点中，而在 HBase 中，负载集中在几个节点中。然而，在性能方面存在很大差异，HBase 每个操作大约需要 62 毫秒，而 MySQL 则超过 2 秒。这种差异也可以通过分区策略来解释，在 MySQL Cluster 中，它基于主键上的哈希函数，由于查询未提供完整的主键，因此它需要访问每个数据节点上的数据，对于每个查询。

*连接工作负载。*

​		接下来的两个查询也是读取密集型的，但这次是基于两个实体的信息，其中一个实体与另一个实例相关联。在第一个查询 SINGLE-JOIN 中，我们使用表 orders 和 orderlines 的连接来获取有关订单及其中项的完整信息。订单是随机选择的，具有均匀分布。在 MySQL 中，这个工作负载执行 50 万次查询，对订单和 orderlines 表执行内连接操作，并返回两个表中所有列的内容。另一方面，从我们的 HBase 转换中，我们以使查询可以在一个单一表中找到相关信息的方式对原始模式进行了去规范化。第二个查询 MULTIPLE-JOIN 是第一个查询的扩展，其中订单行也与产品表进行了一对一的连接。然后，在 MySQL 中执行此查询会连接三个表（订单、订单行和产品）。与前一个查询类似，在 HBase 中，我们从一个单一表中获取所有数据。

​		如预期的那样，从先前工作负载的结果来看，MySQL Cluster 将负载分配给所有节点，而 HBase 仅使用主节点和包含表的指定区域的数据节点。此外，我们发现，在关系数据库中，所需的数据在所有节点上分散，必须在一个节点上进行连接，因此在 MySQL 中，所需传输的数据量约为 HBase 的 2.5 倍（在第一个工作负载中），在第二个工作负载中为 HBase 的 4 倍。由于在 HBase 中使用了预计算的连接，通过去规范化，因此在这些工作负载中的吞吐量要好得多，分别为 439 ops/秒 和 337 ops/秒，而 MySQL Cluster 的吞吐量分别为 54 ops/秒 和 51 ops/秒。

*写入工作负载。*

​		我们使用两个不同的查询评估插入性能，SINGLE-INSERT 和 MULTIPLE-INSERT。第一个工作负载插入 50 万个客户，在 MySQL 中仅涉及对一张表的修改。由于在 HBase 中我们已将表 customers 复制以支持其他访问模式，因此我们需要在三个表中进行插入，它们的区别仅在于行键。第二个工作负载插入 10 万个订单，每个订单包含 4 个订单行。与第一个写入工作负载不同，这次 MySQL 必须在两个表中插入数据，在 HBase 中，由于复制的存在，插入仍然必须在三个表中进行。

​		正如预期的那样，MySQL 的表现优于 HBase。这一次，HBase 将负载分布到三个数据节点中（每个表一个），达到了约 100 ops/秒，而 MySQL 保持了几乎均匀地分配负载到所有数据节点，达到了约 400 ops/秒。尽管如此，对于第二个工作流，性能差异并不明显，MySQL 实现了约 105 ops/秒，而 HBase 实现了 85 ops/秒。性能上的差异显然是由于 HBase 中的复制次数导致的。

*时间扫描工作负载。*

​		此工作负载通过提交 20 万个 TIME-SCAN 查询来测试我们的时间序列编码，以获取在随机一个小时的间隔内放置的一组订单。在 MySQL 中，查询是使用 BETWEEN 比较运算符形成的，在 HBase 中，行键被舍入到最接近的小时。

​		在执行时间扫描工作负载时，我们发现了 MySQL 的 CPU 消耗与其他读取工作负载相比的差异。如图 4 所示，在 MySQL Cluster 中，时间扫描查询的 CPU 利用率增加了 3 倍，这是由于更复杂的索引搜索。另一方面，HBase 保持了与读取工作负载中观察到的相似行为。在这种情况下，HBase 的吞吐量要比 MySQL 好得多，MySQL 的速率约为 30 ops/秒，而 HBase 的执行速率约为 60 ops/秒。

*最近邻工作负载。*最后，我们通过提交 40,000 个查询来测试空间编码，这些查询获取距离随机位置 5 公里范围内的客户。在两种数据库系统中，查询仅使用一张表，在 MySQL 中，查询使用一对 BETWEEN 运算符来定义纬度和经度的坐标。

​		为了执行此工作负载，我们注意到 MySQL 必须将相同的查询复制到每个节点，因为基于哈希的分区不能用于查找存储特定记录的节点。长期来看，对于 MySQL 来说，这种查询的主要瓶颈是数据节点，从所有节点传输查询和结果，如图 5 所示，在数据节点的 CPU 消耗超过了主节点的 CPU 消耗，后者正在等待来自其他节点的查询结果。与仅一个查询的实验不同，在长时间之后，MySQL Cluster 在高负载下执行一个操作的平均时间超过 2 秒，而在 HBase 中仅为 16 毫秒。

![image-20240331145731606](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331145731606.png)

图 4: 时间扫描工作负载的 CPU 利用率。左侧表示 MySQL Cluster 的 CPU 利用率，右侧表示 HBase 的 CPU 利用率。

![image-20240331145805538](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331145805538.png)

图 5: 最近邻查询工作负载的 CPU 利用率。左侧表示 MySQL Cluster 的 CPU 利用率，右侧表示 HBase 的 CPU 利用率。

#### 5.3 查询翻译性能

​		这个实验的目标是比较我们的翻译算法生成的查询性能与由HBase专家编写的查询性能。参与实验的专家是一位有着四年以上HBase工作经验的外部软件工程师，曾为财富500强公司工作。

​		翻译算法被应用于本节开头描述的 SELECT SQL 查询，并将结果代码执行为 Java 程序。类似地，我们要求专家根据 HBase 迁移架构及其与原始关系架构的对应关系提供 SQL 查询的 Java 实现。结果 Java 程序中的每个查询都执行了 3000 次，测量了经过的时间。每次测量报告对应于相同类型的查询的五次运行的平均时间，舍弃前两次运行。为了避免从数据库系统缓存结果，每次提交查询时，谓词中的值都会更改。此外，还比较了查询的结果，以测试两个版本返回的结果的等价性。

​		表 2 显示了自动翻译和专家版本查询的记录时间（以毫秒为单位）。图 6 描述了每种翻译方法的查询平均时间之间的差异。
​		我们的结果表明，自动生成的查询代码的性能与专家编写的代码相当。实质上，两个版本的查询实现遵循相同的结构，唯一的区别是中间变量的减少。
​		例如，在键创建步骤中，以下代码显示了自动生成的代码（顶部）与专家在单行中编写的精简版本（底部）之间的对比。

![image-20240331150040798](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331150040798.png)

​		自动生成翻译中额外变量的原因是为了支持更一般的情况，比如由多个属性组成的行键。然而，中间变量的额外开销少于每个查询 1 毫秒，因此，尽管其冗长度增加，但这证实了翻译算法的良好质量。

![image-20240331150214170](C:\Users\范继腾\AppData\Roaming\Typora\typora-user-images\image-20240331150214170.png)

图 6: 翻译后查询的经过时间。

### 6.结论和未来工作

​		本文讨论了我们将SQL查询迁移到HBase API调用序列的方法论。我们的方法依赖于一组指导原则，将原始的实体关系模型转换为HBase中的多维映射。我们的方法论正式化了一个SQL到HBase查询映射，可以翻译一部分SQL查询，同时保持数据等效性和效率。通过比较专家编写的等效查询的性能，评估了翻译查询的质量，验证了我们翻译的良好质量。最后，我们评估了生成的HBase在性能上与MySQL Cluster的对比，发现其表现优越，从而验证了我们方法论的整体实用性。

​		我们的工作提供了一个全面的关系到HBase翻译过程，包括模式转换方法学、性能评估基准和查询翻译算法。鉴于数据驱动应用的多样性，这个过程只能覆盖迁移问题的一个子集，并且只能部分解决。未来，NoSQL数据库提供的数据建模问题可能是未来研究的重点之一，我们将我们的工作呈现为这个议程的一个彻底的早期步骤。

#### 致谢

作者们要感谢Cybera对其提供快速访问云的慷慨支持。本工作得到了SAVI战略网络的支持。最后，我们要感谢那位慷慨自愿开发代码的专家，我们将其代码与我们自动构建的HBase调用进行了比较。
